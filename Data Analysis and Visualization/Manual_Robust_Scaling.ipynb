{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Robust Scaling Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust scaling is useful for data with outliers, as it is less sensitive to extreme values than standard scaling, which centers data by the mean and scales by the standard deviation. The robust scaling approach centers the data by the median and scales it by the interquartile range (IQR).\n",
    "\n",
    "Given a dataset $ (X) $ with $ (n) $ samples and $ (p) $ features, the robust scaling transformation for each feature $ (X_{j}) $ is defined as:\n",
    "\n",
    "$$\n",
    "X_{j}^{\\text{scaled}} = \\frac{X_{j} - \\text{Median}(X_{j})}{\\text{IQR}(X_{j})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\text{Median}(X_{j}) $ is the median of feature $ X_{j} $,\n",
    "- $ \\text{IQR}(X_{j}) = Q_{3} - Q_{1} $ is the interquartile range of feature $ X_{j} $, where $ Q_{3} $ and $ Q_{1} $ represent the 75th and 25th percentiles of $ X_{j} $, respectively.\n",
    "\n",
    "This scaling ensures that each feature has a median of 0 and a typical spread (IQR) of 1, making the data more robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the median and IQR of a data series\n",
    "def robust_scale(series):\n",
    "    median = np.median(series)\n",
    "    q75, q25 = np.percentile(series, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # Avoid division by zero by adding a small constant\n",
    "    if iqr == 0:\n",
    "        iqr = 1e-9\n",
    "    \n",
    "    # Apply robust scaling\n",
    "    return (series - median) / iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory and columns to be dropped\n",
    "DIRECTORY = 'hopkins_export/'\n",
    "drop_ls = [\n",
    "    \"expected_time\",\n",
    "    \"flip_time\",\n",
    "    \"stim_pos\",\n",
    "    \"user_pos\",\n",
    "    \"lambda_val\",\n",
    "    \"change_rate_x\",\n",
    "]\n",
    "\n",
    "# List of subject files in the directory\n",
    "subject_files = [f for f in os.listdir(DIRECTORY) if f.endswith('.csv')]\n",
    "\n",
    "# Process each subject file\n",
    "for subject_file in subject_files:\n",
    "    # Load each subject dataset\n",
    "    subject_data = pd.read_csv(os.path.join(DIRECTORY, subject_file))\n",
    "    \n",
    "    # Separate columns to be scaled and columns to keep as is\n",
    "    subject_data_drop = subject_data.drop(columns=drop_ls)\n",
    "    subject_data_keep = subject_data[drop_ls]\n",
    "    \n",
    "    # Manually apply robust scaling\n",
    "    subject_scaled = subject_data_drop.apply(robust_scale)\n",
    "    \n",
    "    # Combine scaled and unscaled columns, preserving original column order\n",
    "    subject_final = pd.concat([subject_data_keep, subject_scaled], axis=1)\n",
    "    subject_final = subject_final[subject_data.columns]\n",
    "    \n",
    "    # Save the scaled data to a new CSV file\n",
    "    subject_final.to_csv(f'Manual_Robust_Scaling_{subject_file}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
